{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Install keypoint MoSeq","metadata":{"id":"aaab50c4"}},{"cell_type":"code","source":"! pip install keypoint-moseq","metadata":{"id":"84990f8c","outputId":"faf96656-4f03-4041-89c3-04a6324310a6","tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport requests, zipfile\nimport json\nimport numpy as np\nimport pandas as pd\nimport keypoint_moseq as kpms","metadata":{"id":"8yWxmVOantV7","outputId":"398856ed-1578-45f1-a24a-85ff46dc57d9","tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Project setup\nCreate a new project directory with a keypoint-MoSeq `config.yml` file.","metadata":{"id":"f8a52043"}},{"cell_type":"code","source":"data_dir = './data'\nos.makedirs(data_dir, exist_ok=True)\nproject_dir = './results'\nos.makedirs(project_dir, exist_ok=True)","metadata":{"id":"intermediate-kenya","outputId":"ab590783-9a4f-4e21-9fa5-7fe0a27556b4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Option: Manual setup","metadata":{"id":"b9e62b8e"}},{"cell_type":"code","source":"bodyparts=['nose', 'left ear', 'right ear', 'neck', 'left hip', 'right hip', 'tail base']\n\nskeleton=[\n     ['tail base', 'left hip'],\n     ['tail base', 'right hip'],\n     ['right hip', 'neck'],\n     ['left hip', 'neck'],\n     ['left ear', 'neck'],\n     ['right ear', 'neck'],\n     ['left ear', 'nose'],\n     ['right ear', 'nose']]\n\nvideo_dir = os.path.join(data_dir, 'videos')\n\nkpms.setup_project(\n     project_dir,\n     video_dir=video_dir,\n     bodyparts=bodyparts,\n     skeleton=skeleton,\n     overwrite=True)","metadata":{"id":"0d804ac5","mystnb":{"code_prompt_hide":"Custom setup","code_prompt_show":"Custom setup"},"tags":["hide-cell"],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Edit the config file","metadata":{"id":"gothic-viking"}},{"cell_type":"markdown","source":"The config can be edited in a text editor or using the function `kpms.update_config`, as shown below. In general, the following parameters should be specified for each project:\n\n- `bodyparts` (name of each keypoint; automatically imported from SLEAP/DeepLabCut)\n- `use_bodyparts` (subset of bodyparts to use for modeling, set to all bodyparts by default; for mice we recommend excluding the tail)\n- `anterior_bodyparts` and `posterior_bodyparts` (used for rotational alignment)\n- `video_dir` (directory with videos of each experiment)\n\nEdit the config as follows for the [example DeepLabCut dataset](https://drive.google.com/drive/folders/1UNHQ_XCQEKLPPSjGspRopWBj6-YNDV6G?usp=share_link):","metadata":{"id":"sfRK9S5PRyS4"}},{"cell_type":"code","source":"kpms.update_config(\n    project_dir,\n    video_dir=os.path.join(data_dir, 'videos'),\n    anterior_bodyparts=['nose'],\n    posterior_bodyparts=['tail base'],\n    use_bodyparts=['nose', 'left ear', 'right ear', 'neck', 'left hip', 'right hip', 'tail base'],\n    overwrite=True\n    )\nconfig = lambda: kpms.load_config(project_dir)","metadata":{"id":"theoretical-yahoo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download the dataset ðŸ“²","metadata":{"execution":{},"id":"Inud7Rud4Lni"}},{"cell_type":"markdown","source":"The CalMS21 dataset is hosted by [Caltech](https://data.caltech.edu/records/1991). For now, we'll focus on the Task 1 data, which can be downloaded as follows:","metadata":{"id":"X4xYEHUmVR5C"}},{"cell_type":"code","source":"# Download and unzip the data\nfname = 'task1_classic_classification.zip'\nurl = \"https://data.caltech.edu/records/s0vdx-0k302/files/task1_classic_classification.zip?download=1\"\n\nif not os.path.isfile(fname):\n  try:\n    r = requests.get(url)\n  except requests.ConnectionError:\n    print(\"!!! Failed to download data !!!\")\n  else:\n    if r.status_code != requests.codes.ok:\n      print(\"!!! Failed to download data !!!\")\n    else:\n      with open(fname, \"wb\") as fid:\n        fid.write(r.content)\nelse:\n  print('Data have already been downloaded!!!')\n\nif not os.path.exists('task1_classic_classification'):\n  # Unzip the file\n  with zipfile.ZipFile(fname, 'r') as zip_ref:\n    zip_ref.extractall('.')\n\n\n# Download the script\nfname = 'calms21_convert_to_npy.py'\nurl = \"https://data.caltech.edu/records/s0vdx-0k302/files/calms21_convert_to_npy.py?download=1\"\n\nif not os.path.isfile(fname):\n  try:\n    r = requests.get(url)\n  except requests.ConnectionError:\n    print(\"!!! Failed to download data !!!\")\n  else:\n    if r.status_code != requests.codes.ok:\n      print(\"!!! Failed to download data !!!\")\n    else:\n      with open(fname, \"wb\") as fid:\n        fid.write(r.content)","metadata":{"cellView":"form","id":"yxfyV1qb4Lni","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset files are stored as json files. For ease of handling, we'll first convert them to .npy files using the script we just downloaded, `calms21_convert_to_npy.py`. The output of this script is a pair of files, `calms21_task1_train.npy` and `calms21_task1_test.npy`.\n\nIf you include the optional `parse_treba` flag, the script will create files `calms21_task1_train_features.npy` and `calms21_task1_test_features.npy`, which contain 32 features created using <a href=https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Task_Programming_Learning_Data_Efficient_Behavior_Representations_CVPR_2021_paper.html>Task Programming</a>.\n\n","metadata":{"execution":{},"id":"Ufp9XeX94Lni"}},{"cell_type":"code","source":"!python calms21_convert_to_npy.py  --input_directory '.' --output_directory '.'","metadata":{"id":"MRCB-p-m4Lni","outputId":"92448d1d-93f6-493d-ee6b-6f587bd7aa49","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the data","metadata":{"execution":{},"id":"3ep1aIoc4Lnj"}},{"cell_type":"markdown","source":"The following loader function can be used to unpack the `.npy` files containing your train and test sets.","metadata":{"id":"RN1ipakzVX1f"}},{"cell_type":"code","source":"def load_task1_data(data_path):\n  \"\"\"\n  Load data for task 1:\n      The vocaubulary tells you how to map behavior names to class ids;\n      it is the same for all sequences in this dataset.\n  \"\"\"\n  data_dict = np.load(data_path, allow_pickle=True).item()\n  dataset = data_dict['annotator-id_0']\n  # Get any sequence key.\n  sequence_id = list(data_dict['annotator-id_0'].keys())[0]\n  vocabulary = data_dict['annotator-id_0'][sequence_id]['metadata']['vocab']\n  return dataset, vocabulary","metadata":{"id":"x0zulBeE4Lnj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load full dataset\ntraining_data_full, vocab = load_task1_data('./calms21_task1_train.npy')\ntest_data_full, _ = load_task1_data('./calms21_task1_test.npy')\n\n# Select \nn_train = 50\nn_test = 10\n\ntraining_keys = list(training_data_full.keys())\ntraining_data = {k: training_data_full[k] for k in training_keys[:n_train]}\n\ntest_keys = list(test_data_full.keys())\ntest_data = {k: test_data_full[k] for k in test_keys[:n_test]}\n","metadata":{"id":"OAJ_Y8454Lnj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset Specifications","metadata":{"execution":{},"id":"vqw2-kR74Lnj"}},{"cell_type":"markdown","source":"`training_data` and `test_data` are both dictionaries with a key for each Sequence in the dataset, where a Sequence is a single resident-intruder assay. Each Sequence contains the following fields:\n\n<ul>\n<li><b>keypoints</b>: tracked locations of body parts on the two interacting mice. These are produced using a Stacked Hourglass network trained on 15,000 hand-labeled frames.\n<ul>\n<li>Dimensions: (# frames) x (mouse ID) x (x, y coordinate) x (body part).\n<li>Units: pixels; coordinates are relative to the entire image. Original image dimensions are 1024 x 570.\n</ul>\n<li><b>scores</b>: confidence estimates for the tracked keypoints.\n<ul>\n<li>Dimensions: (# frames) x (mouse ID) x (body part).\n<li>Units: unitless, range 0 (lowest confidence) to 1 (highest confidence).\n</ul>\n<li> <b>annotations</b>: behaviors id as an integer annotated at each frame by a domain expert. See below for the behavior id to behavior name mappings.\n<ul>\n<li>Dimensions: (# frames) .\n</ul>\n<li><b>metadata</b>: The recorded metadata is annotator_id which is represented by an int, and the vocab, containing a dictionary which maps behavior names to integer ids in annotations.\n</ul>\n\nThe 'taskprog_features' file contains the additional field:\n\n<ul>\n<li><b>features</b>: pre-computed features from a model trained with task programming on the trajectory data of the CalMS21 unlabeled videos set.\n<ul>\n<li>Dimensions: (# frames) x (feature dim = 32).\n</li>\n</ul>\n</ul>\n\n<b>NOTE:</b> for all keypoints, mouse 0 is the resident (black) mouse and mouse 1 is the intruder (white) mouse. There are 7 tracked body parts, ordered (nose, left ear, right ear, neck, left hip, right hip, tail base).","metadata":{"id":"AWQhPrjHUouZ"}},{"cell_type":"markdown","source":"### Data overview","metadata":{}},{"cell_type":"markdown","source":"As described above, our dataset consists of train and test sets, which are both dictionaries of Sequences, and an accompanying vocabulary telling us which behavior is which:","metadata":{"id":"cabE59-tU0Z7"}},{"cell_type":"code","source":"print(\"Sample dataset keys: \", list(training_data.keys())[:3])\nprint(\"Vocabulary: \", vocab)\nprint(\"Number of train Sequences: \", len(training_data))\nprint(\"Number of test Sequences: \", len(test_data))","metadata":{"id":"wJycOthP4Lnj","outputId":"f3f12649-bd7c-4e40-f721-a955bfc0706d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Sample overview","metadata":{"execution":{},"id":"RT0wpWF94Lnj"}},{"cell_type":"markdown","source":"Next let's take a look at one example Sequence:","metadata":{"id":"dIxO4QnuVEFO"}},{"cell_type":"code","source":"sequence_names = list(training_data.keys())\nsample_sequence_key = sequence_names[0]\nsingle_sequence = training_data[sample_sequence_key]\nprint(\"Name of our sample sequence: \", sample_sequence_key)\nprint(\"Sequence keys: \", single_sequence.keys())\nprint(\"Sequence metadata: \", single_sequence['metadata'])\nprint(f\"Number of Frames in Sequence \\\"{sample_sequence_key}\\\": \", len(single_sequence['annotations']))\nprint(f\"Keypoints data shape of Sequence \\\"{sample_sequence_key}\\\": \", single_sequence['keypoints'].shape)","metadata":{"id":"W-43rSHb4Lnj","outputId":"c1e4baf5-d126-4779-a2be-383fdf98efd8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reshaping for Keypoint-Moseq input","metadata":{"id":"AsxnGdzFC6Ua"}},{"cell_type":"markdown","source":"Keypoint-moseq model requires 3 inputs: Coordinates, confidences, bodyparts","metadata":{"id":"yCh_F9JBDs5Z"}},{"cell_type":"markdown","source":"### Testing/Experimenting","metadata":{"id":"41mR9WmeLD4Z"}},{"cell_type":"markdown","source":"#### Coordinates Input: Taking One sample and trying to see if we can reshape it to the shape of the input for keypoint-moseq:","metadata":{"id":"LX0J3xRdBeqM"}},{"cell_type":"markdown","source":"Required Input: coordinates array of shape: (no. of frames x body part x no. of coordinates)","metadata":{"id":"I_1tC9SHE7fk"}},{"cell_type":"code","source":"training_data['task1/train/mouse001_task1_annotator1'].keys()","metadata":{"id":"jc2BABAz_8BO","outputId":"47370d30-abc0-4cf1-d21b-8fc91a188169","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Shape of 'keypoints' of 1 sequence\nsamp_seq = training_data['task1/train/mouse001_task1_annotator1']['keypoints']\nsamp_seq.shape","metadata":{"id":"j4AAOcnO6cHY","outputId":"9f90b703-8806-4834-d8a4-579101cb104a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"samp_seq[0,0,0,:] #First frame, resident mouse, x axis coordinate, all body parts\n#First index: frame number\n#Second index: mouse ID - 0:resident ; 1:intruder\n#Third index: coordinate axis  -  0:x-axis ; 1:y-axis\n#Fourth index: body part  -  (nose, left ear, right ear, neck, left hip, right hip, tail base)","metadata":{"id":"GbQHjJqt7eaS","outputId":"ef852006-9e18-47cc-bdf6-1d77902ff4e3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Reshaping one of the assays to the way required by keypoint-moseq\n\n#Splitting resident and intruder mouse keypoints:\nsamp_seq_resident = samp_seq[:,0,:,:]\nsamp_seq_intruder = samp_seq[:,1,:,:]\n\n#Reshaping it to: frame x body part x coordinate  (no. of frames, 7, 2)\nsamp_seq_resident = np.transpose(samp_seq_resident, (0, 2, 1))\nsamp_seq_resident.shape","metadata":{"id":"mbpD4xnc6sGF","outputId":"a9448be8-4e39-43ed-f424-411708b2588d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Confidences Input: For 1 assay","metadata":{"id":"6LjVV54vD8cT"}},{"cell_type":"markdown","source":"Required input: confidence array of shape: ( no. of frame x body part )\n\n.Confidence score range: 0 to 1","metadata":{"id":"EyN9aPVJEq2-"}},{"cell_type":"code","source":"# Shape of 'scores' of 1 sequence\nconf_seq = training_data['task1/train/mouse001_task1_annotator1']['scores']\nconf_seq.shape","metadata":{"id":"fgaxZCVOEFbw","outputId":"51b30b27-ed0e-4ae4-bf19-88b93a48fbc2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reshaping:\n\n#splitting resident and intruder\nconf_seq_res = conf_seq[:,0,:]\nconf_seq_int = conf_seq[:,1,:]\n\nconf_seq_res.shape","metadata":{"id":"ZFYDu5w7FUTj","outputId":"cf6d35d3-f9de-43e7-99f3-d4e13e71b4a8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Bodyparts Input","metadata":{"id":"3_O41b_ZF7X5"}},{"cell_type":"code","source":"bodyparts = ['nose', 'left ear', 'right ear', 'neck', 'left hip', 'right hip', 'tail base']\nbodyparts","metadata":{"id":"kHvWTTgKGBU-","outputId":"c595902d-3fe0-4507-e075-a3482ccb8011","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Final: Getting the 3 inputs for the keypoint-moseq model**","metadata":{"id":"JdocKmQ1BqNy"}},{"cell_type":"code","source":"## Creating a Dictionary that maps recordings to arrays of shape (num_frames, num_keypoints, num_dimensions) for resident and intruder mouse\n## This will be the 'coordinates' input for the keypoint-moseq model\n\ncoordinates_resident = {}\nconfidences_resident = {}\ncoordinates_intruder = {}\nconfidences_intruder = {}\n\nfor i in training_data.keys():\n\n  coord_data = training_data[i]['keypoints']\n  conf_data = training_data[i]['scores']\n\n  reshaped_coord_data_res = np.transpose(coord_data[:,0,:,:], (0,2,1))\n  reshaped_coord_data_intr = np.transpose(coord_data[:,1,:,:], (0,2,1))\n  reshaped_conf_data_res = conf_data[:,0,:]\n  reshaped_conf_data_intr = conf_data[:,1,:]\n\n  coordinates_resident[i] = reshaped_coord_data_res\n  coordinates_intruder[i] = reshaped_coord_data_intr\n  confidences_resident[i] = reshaped_conf_data_res\n  confidences_intruder[i] = reshaped_conf_data_intr","metadata":{"id":"somy9cxJ-TTB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bodyparts_list = ['nose', 'left ear', 'right ear', 'neck', 'left hip', 'right hip', 'tail base']\nbodyparts","metadata":{"id":"nSrX1uncKwAJ","outputId":"c20d6dfe-a60c-4b47-e8bf-f69e7fef63cf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load data for the model","metadata":{"id":"WChc4Mt1XQsw"}},{"cell_type":"code","source":"coordinates, confidences, bodyparts = coordinates_resident, confidences_resident, bodyparts_list\n\n# format data for the model\ndata, metadata = kpms.format_data(coordinates, confidences, **config())","metadata":{"id":"KAmgkO2mXX1n","outputId":"93f5895a-f257-4d59-f2c1-8c720c982ed3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fit PCA\n\nRun the cell below to fit a PCA model to aligned and centered keypoint coordinates.\n\n- The model is saved to ``{project_dir}/pca.p`` and can be reloaded using ``kpms.load_pca``.\n- Two plots are generated: a cumulative [scree plot](https://en.wikipedia.org/wiki/Scree_plot) and a depiction of each PC, where translucent nodes/edges represent the mean pose and opaque nodes/edges represent a perturbation in the direction of the PC.\n- After fitting, edit `latent_dimension` in the config. This determines the dimension of the pose trajectory used to fit keypoint-MoSeq. A good heuristic is the number of dimensions needed to explain 90% of variance, or 10 dimensions - whichever is lower.  ","metadata":{"id":"organizational-theorem"}},{"cell_type":"code","source":"pca = kpms.fit_pca(**data, **config())\nkpms.save_pca(pca, project_dir)\n\nkpms.print_dims_to_explain_variance(pca, 0.9)\nkpms.plot_scree(pca, project_dir=project_dir)\nkpms.plot_pcs(pca, project_dir=project_dir, **config())\n\n# use the following to load an already fit model\n# pca = kpms.load_pca(project_dir)","metadata":{"id":"respiratory-canvas","outputId":"67c62bdb-0420-47ae-9523-6b1c0a199ff0","tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After fitting, edit latent_dimension in the config.\n# This determines the dimension of the pose trajectory used to fit keypoint-MoSeq.\n# A good heuristic is the number of dimensions needed to explain 90% of variance, or 10 dimensions - whichever is lower.\n\n# I have updated latent_dim as 7 since 90% variance was explained by 7 dimensions\nkpms.update_config(project_dir, latent_dim=7)","metadata":{"id":"a1a3a9b6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model fitting\n\nFitting a keypoint-MoSeq model involves:\n1. **Initialization:** Auto-regressive (AR) parameters and syllable sequences are randomly initialized using pose trajectories from PCA.\n2. **Fitting an AR-HMM:** The AR parameters, transition probabilities and syllable sequences are iteratively updated through Gibbs sampling.\n3. **Fitting the full model:** All parameters, including both the AR-HMM as well as centroid, heading, noise-estimates and continuous latent states (i.e. pose trajectories) are iteratively updated through Gibbs sampling. This step is especially useful for noisy data.\n4. **Extracting model results:** The learned states of the model are parsed and saved to disk for vizualization and downstream analysis.\n4. **[Optional] Applying the trained model:** The learned model parameters can be used to infer a syllable sequences for additional data.\n\n## Setting kappa\n\nMost users will need to adjust the **kappa** hyperparameter to achieve the desired distribution of syllable durations. For this tutorial we chose kappa values that yielded a median syllable duration of 400ms (12 frames). Most users will need to tune kappa to their particular dataset. Higher values of kappa lead to longer syllables. **You will need to pick two kappas: one for AR-HMM fitting and one for the full model.**\n- We recommend iteratively updating kappa and refitting the model until the target syllable time-scale is attained.  \n- Model fitting can be stopped at any time by interrupting the kernel, and then restarted with a new kappa value.\n- The full model will generally require a lower value of kappa to yield the same target syllable durations.\n- To adjust the value of kappa in the model, use `kpms.update_hypparams` as shown below. Note that this command only changes kappa in the model dictionary, not the kappa value in the config file. The value in the config is only used during model initialization.","metadata":{"id":"accomplished-pantyhose"}},{"cell_type":"markdown","source":"## Initialization","metadata":{"id":"utility-penetration"}},{"cell_type":"code","source":"# initialize the model\nmodel = kpms.init_model(data, pca=pca, **config())\n\n# optionally modify kappa\n# model = kpms.update_hypparams(model, kappa=NUMBER)","metadata":{"id":"found-administrator","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fitting an AR-HMM\n\nIn addition to fitting an AR-HMM, the function below:\n- generates a name for the model and a corresponding directory in `project_dir`\n- saves a checkpoint every 25 iterations from which fitting can be restarted\n- plots the progress of fitting every 25 iterations, including\n    - the distributions of syllable frequencies and durations for the most recent iteration\n    - the change in median syllable duration across fitting iterations\n    - a sample of the syllable sequence across iterations in a random window","metadata":{"id":"partial-remove"}},{"cell_type":"code","source":"num_ar_iters = 50\n\nmodel, model_name = kpms.fit_model(\n    model, data, metadata, project_dir,\n    ar_only=True, num_iters=num_ar_iters)","metadata":{"id":"888e6ff7","outputId":"af1d063a-07e5-478a-ed73-7132d7175109","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fitting the full model\n\nThe following code fits a full keypoint-MoSeq model using the results of AR-HMM fitting for initialization. If using your own data, you may need to try a few values of kappa at this step.","metadata":{"id":"thorough-identity"}},{"cell_type":"code","source":"# load model checkpoint\nmodel, data, metadata, current_iter = kpms.load_checkpoint(\n    project_dir, model_name, iteration=num_ar_iters)\n\n# modify kappa to maintain the desired syllable time-scale\nmodel = kpms.update_hypparams(model, kappa=1e4)\n\n# run fitting for an additional 500 iters\nmodel = kpms.fit_model(\n    model, data, metadata, project_dir, model_name, ar_only=False,\n    start_iter=current_iter, num_iters=current_iter+500)[0]","metadata":{"id":"swiss-repeat","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sort syllables by frequency\n\nPermute the states and parameters of a saved checkpoint so that syllables are labeled in order of frequency (i.e. so that `0` is the most frequent, `1` is the second most, and so on).","metadata":{"id":"0837e0ad"}},{"cell_type":"code","source":"# modify a saved checkpoint so syllables are ordered by frequency\nkpms.reindex_syllables_in_checkpoint(project_dir, model_name);","metadata":{"id":"902ccabf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```{warning}\nReindexing is only applied to the checkpoint file. Therefore, if you perform this step after extracting the modeling results or generating vizualizations, then those steps must be repeated.\n```","metadata":{"id":"bc027d4a"}},{"cell_type":"markdown","source":"## Extract model results\n\nParse the modeling results and save them to `{project_dir}/{model_name}/results.h5`. The results are stored as follows, and can be reloaded at a later time using `kpms.load_results`. Check the docs for an [in-depth explanation of the modeling results](https://keypoint-moseq.readthedocs.io/en/latest/FAQs.html#interpreting-model-outputs).\n```\n    results.h5\n    â”œâ”€â”€recording_name1\n    â”‚  â”œâ”€â”€syllable      # syllable labels (z)\n    â”‚  â”œâ”€â”€latent_state  # inferred low-dim pose state (x)\n    â”‚  â”œâ”€â”€centroid      # inferred centroid (v)\n    â”‚  â””â”€â”€heading       # inferred heading (h)\n    â‹®\n```","metadata":{"id":"79951b99"}},{"cell_type":"code","source":"import h5py\n# load the most recent model checkpoint\nmodel, data, metadata, current_iter = kpms.load_checkpoint(project_dir, model_name)\n\n# extract results\n# results = kpms.extract_results(model, metadata, project_dir, model_name)\n# results = kpms.extract_results(model, metadata, project_dir, model_name, save_results=True)\nresults_file = os.path.join(project_dir, model_name, \"results.h5\")\nif os.path.exists(results_file):\n    os.remove(results_file)\n\nresults = kpms.extract_results(model, metadata, project_dir, model_name, save_results=True)","metadata":{"id":"d8abffb5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Optional] Save results to csv\n\nAfter extracting to an h5 file, the results can also be saved as csv files. A separate file will be created for each recording and saved to `{project_dir}/{model_name}/results/`.","metadata":{"id":"a37f9d42"}},{"cell_type":"code","source":"# optionally save results as csv\nkpms.save_results_as_csv(results, project_dir, model_name)","metadata":{"id":"0ceea1e2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization","metadata":{"id":"breeding-fashion"}},{"cell_type":"markdown","source":"## Trajectory plots\nGenerate plots showing the median trajectory of poses associated with each given syllable.","metadata":{"id":"a2491a0d"}},{"cell_type":"code","source":"kpms.generate_trajectory_plots(coordinates, results, project_dir, model_name, **config())","metadata":{"id":"subject-disney","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Syllable Dendrogram\nPlot a dendrogram representing distances between each syllable's median trajectory.","metadata":{"id":"d670667d"}},{"cell_type":"code","source":"kpms.plot_similarity_dendrogram(coordinates, results, project_dir, model_name, **config())","metadata":{"id":"81a324c4","trusted":true},"outputs":[],"execution_count":null}]}